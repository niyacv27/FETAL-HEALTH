{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WddA8erci4Al"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fCll6M-hmayU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHH-YTU6m8uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eGPpmid8ndPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sbE5Z6lqst7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5solZIirog-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"fetal_health.csv\")\n",
        "X = df.iloc[:, :-1].to_numpy()\n",
        "y = df.iloc[:, -1].to_numpy().astype(int)\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "y_resampled = y_resampled.astype(int)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Manual implementation of K-Nearest Neighbors (KNN)\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for x in X_test:\n",
        "            distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "            most_common = Counter(k_nearest_labels).most_common(1)\n",
        "            predictions.append(most_common[0][0])\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Manual implementation of Decision Tree used in Random Forest\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1]\n",
        "        self.tree = self._grow_tree(X, y, depth=0)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth):\n",
        "        num_samples, num_features = X.shape\n",
        "        if depth >= self.max_depth or len(set(y)) == 1:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        best_feat, best_thresh = self._best_split(X, y)\n",
        "        if best_feat is None:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        left_idx = X[:, best_feat] < best_thresh\n",
        "        right_idx = ~left_idx\n",
        "        left_subtree = self._grow_tree(X[left_idx], y[left_idx], depth + 1)\n",
        "        right_subtree = self._grow_tree(X[right_idx], y[right_idx], depth + 1)\n",
        "\n",
        "        return (best_feat, best_thresh, left_subtree, right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "        for i in range(self.n_features):\n",
        "            thresholds = np.unique(X[:, i])\n",
        "            for threshold in thresholds:\n",
        "                left_idx = X[:, i] < threshold\n",
        "                right_idx = ~left_idx\n",
        "                if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
        "                    continue\n",
        "                gain = self._information_gain(y, y[left_idx], y[right_idx])\n",
        "                if gain > best_gain:\n",
        "                    best_gain, split_idx, split_threshold = gain, i, threshold\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _information_gain(self, parent, left, right):\n",
        "        def entropy(y):\n",
        "            probs = np.bincount(y) / len(y)\n",
        "            return -np.sum([p * np.log2(p) for p in probs if p > 0])\n",
        "\n",
        "        n = len(parent)\n",
        "        return entropy(parent) - (len(left) / n * entropy(left) + len(right) / n * entropy(right))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if not isinstance(node, tuple):\n",
        "            return node\n",
        "        feature_idx, threshold, left, right = node\n",
        "        return self._traverse_tree(x, left) if x[feature_idx] < threshold else self._traverse_tree(x, right)\n",
        "\n",
        "# Manual implementation of Gradient Boosting\n",
        "class GradientBoosting:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.initial_prediction = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.initial_prediction = np.mean(y)\n",
        "        residuals = y - self.initial_prediction\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.full(X.shape[0], self.initial_prediction)\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "# Manual implementation of XGBoost\n",
        "class XGBoost:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        residuals = y.copy()\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.zeros(X.shape[0])\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "\n",
        "# Manual implementation of Ensemble Model with Majority Voting\n",
        "class EnsembleModel:\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for model in self.models:\n",
        "            model.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([model.predict(X) for model in self.models])\n",
        "        return np.array([Counter(predictions[:, i]).most_common(1)[0][0] for i in range(len(X))])\n",
        "\n",
        "# Initialize models\n",
        "knn = KNN(k=3)\n",
        "dt = DecisionTree(max_depth=5)\n",
        "gb = GradientBoosting(n_estimators=50, learning_rate=0.1, max_depth=3)\n",
        "xgb = XGBoost(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "\n",
        "# Create ensemble model\n",
        "ensemble = EnsembleModel(models=[knn, dt, gb,xgb])\n",
        "\n",
        "# Train and evaluate models\n",
        "models = {'KNN': knn, 'Decision Tree': dt, 'Gradient Boosting': gb, 'XGBoost':xgb, 'Ensemble': ensemble}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "vNhUk9zwswtw",
        "outputId": "be3dce6d-94ac-4eba-b258-5b74a86ccb9b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 0.9507\n",
            "Decision Tree Accuracy: 0.8963\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fc0bf8c2fa23>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'KNN'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Decision Tree'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gradient Boosting'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'XGBoost'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ensemble'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fc0bf8c2fa23>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresiduals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mresiduals\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fc0bf8c2fa23>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fc0bf8c2fa23>\u001b[0m in \u001b[0;36m_grow_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mbest_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_best_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_feat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fc0bf8c2fa23>\u001b[0m in \u001b[0;36m_best_split\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_information_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_gain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mbest_gain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fc0bf8c2fa23>\u001b[0m in \u001b[0;36m_information_gain\u001b[0;34m(self, parent, left, right)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fc0bf8c2fa23>\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_information_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "xbQQc3wQun-T",
        "outputId": "1e39ef5c-29b4-4c79-b456-5b297d7828bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 0.9507\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5cc2a7881102>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'KNN'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Decision Tree'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gradient Boosting'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'XGBoost'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ensemble'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5cc2a7881102>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5cc2a7881102>\u001b[0m in \u001b[0;36m_grow_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mbest_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_best_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_feat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5cc2a7881102>\u001b[0m in \u001b[0;36m_best_split\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_information_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_gain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                     \u001b[0mbest_gain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"fetal_health.csv\")\n",
        "X = df.iloc[:, :-1].to_numpy()\n",
        "y = df.iloc[:, -1].to_numpy().astype(int)\n",
        "\n",
        "# Display class distribution before SMOTE\n",
        "print(\"Class distribution before SMOTE:\", Counter(y))\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "y_resampled = y_resampled.astype(int)\n",
        "\n",
        "# Display class distribution after SMOTE\n",
        "print(\"Class distribution after SMOTE:\", Counter(y_resampled))\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Manual implementation of K-Nearest Neighbors (KNN)\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for x in X_test:\n",
        "            distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "            most_common = Counter(k_nearest_labels).most_common(1)\n",
        "            predictions.append(most_common[0][0])\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Manual implementation of Decision Tree used in Random Forest\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1]\n",
        "        self.tree = self._grow_tree(X, y, depth=0)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth):\n",
        "        num_samples, num_features = X.shape\n",
        "        if depth >= self.max_depth or len(set(y)) == 1:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        best_feat, best_thresh = self._best_split(X, y)\n",
        "        if best_feat is None:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        left_idx = X[:, best_feat] < best_thresh\n",
        "        right_idx = ~left_idx\n",
        "        left_subtree = self._grow_tree(X[left_idx], y[left_idx], depth + 1)\n",
        "        right_subtree = self._grow_tree(X[right_idx], y[right_idx], depth + 1)\n",
        "\n",
        "        return (best_feat, best_thresh, left_subtree, right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "        for i in range(self.n_features):\n",
        "            thresholds = np.unique(X[:, i])\n",
        "            for threshold in thresholds:\n",
        "                left_idx = X[:, i] < threshold\n",
        "                right_idx = ~left_idx\n",
        "                if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
        "                    continue\n",
        "                gain = self._information_gain(y, y[left_idx], y[right_idx])\n",
        "                if gain > best_gain:\n",
        "                    best_gain, split_idx, split_threshold = gain, i, threshold\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "      def _information_gain(self, parent, left, right):\n",
        "        def entropy(y):\n",
        "           y = y.astype(int)  # Convert y to integers\n",
        "           probs = np.bincount(y) / len(y)\n",
        "           return -np.sum([p * np.log2(p) for p in probs if p > 0])\n",
        "\n",
        "        n = len(parent)\n",
        "        return entropy(parent) - (len(left) / n * entropy(left) + len(right) / n * entropy(right))\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if not isinstance(node, tuple):\n",
        "            return node\n",
        "        feature_idx, threshold, left, right = node\n",
        "        return self._traverse_tree(x, left) if x[feature_idx] < threshold else self._traverse_tree(x, right)\n",
        "\n",
        "# Manual implementation of Random Forest\n",
        "class RandomForest:\n",
        "    def __init__(self, n_estimators=10, max_depth=5, max_features=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.max_features = max_features\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            indices = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_sample, y_sample = X[indices], y[indices]\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.array([Counter(tree_predictions[:, i]).most_common(1)[0][0] for i in range(len(X))])\n",
        "\n",
        "# Manual implementation of Gradient Boosting\n",
        "class GradientBoosting:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.initial_prediction = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.initial_prediction = np.mean(y)\n",
        "        residuals = y - self.initial_prediction\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.full(X.shape[0], self.initial_prediction)\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "# Manual implementation of XGBoost\n",
        "class XGBoost:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        residuals = y.copy()\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.zeros(X.shape[0])\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "        # Manual implementation of Ensemble Model with Majority Voting\n",
        "class EnsembleModel:\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for model in self.models:\n",
        "            model.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([model.predict(X) for model in self.models])\n",
        "        return np.array([Counter(predictions[:, i]).most_common(1)[0][0] for i in range(len(X))])\n",
        "\n",
        "# Initialize models\n",
        "knn = KNN(k=3)\n",
        "rf = RandomForest(n_estimators=10, max_depth=5, max_features=None)\n",
        "gb = GradientBoosting(n_estimators=50, learning_rate=0.1, max_depth=3)\n",
        "xgb = XGBoost(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "\n",
        "# Create ensemble model\n",
        "ensemble = EnsembleModel(models=[knn, dt, gb,xgb])\n",
        "\n",
        "# Train and evaluate models\n",
        "models = {'KNN': knn, 'RandomForest':rf, 'Gradient Boosting': gb, 'XGBoost': xgb, 'Ensemble': ensemble}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "6A_xKaWvwkvH",
        "outputId": "91cb8038-9519-4eb2-84ae-a85b0eaefb3f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 95)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m95\u001b[0m\n\u001b[0;31m    def _information_gain(self, parent, left, right):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"fetal_health.csv\")\n",
        "X = df.iloc[:, :-1].to_numpy()\n",
        "y = df.iloc[:, -1].to_numpy().astype(int)\n",
        "\n",
        "# Display class distribution before SMOTE\n",
        "print(\"Class distribution before SMOTE:\", Counter(y))\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "y_resampled = y_resampled.astype(int)\n",
        "\n",
        "# Display class distribution after SMOTE\n",
        "print(\"Class distribution after SMOTE:\", Counter(y_resampled))\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Manual implementation of K-Nearest Neighbors (KNN)\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for x in X_test:\n",
        "            distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "            most_common = Counter(k_nearest_labels).most_common(1)\n",
        "            predictions.append(most_common[0][0])\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Manual implementation of Decision Tree used in Random Forest\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1]\n",
        "        self.tree = self._grow_tree(X, y, depth=0)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth):\n",
        "        num_samples, num_features = X.shape\n",
        "        if depth >= self.max_depth or len(set(y)) == 1:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        best_feat, best_thresh = self._best_split(X, y)\n",
        "        if best_feat is None:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        left_idx = X[:, best_feat] < best_thresh\n",
        "        right_idx = ~left_idx\n",
        "        left_subtree = self._grow_tree(X[left_idx], y[left_idx], depth + 1)\n",
        "        right_subtree = self._grow_tree(X[right_idx], y[right_idx], depth + 1)\n",
        "\n",
        "        return (best_feat, best_thresh, left_subtree, right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "        for i in range(self.n_features):\n",
        "            thresholds = np.unique(X[:, i])\n",
        "            for threshold in thresholds:\n",
        "                left_idx = X[:, i] < threshold\n",
        "                right_idx = ~left_idx\n",
        "                if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
        "                    continue\n",
        "                gain = self._information_gain(y, y[left_idx], y[right_idx])\n",
        "                if gain > best_gain:\n",
        "                    best_gain, split_idx, split_threshold = gain, i, threshold\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _information_gain(self, parent, left, right):\n",
        "        def entropy(y):\n",
        "            y = np.array(y, dtype=int)  # Ensure integer values\n",
        "            y = y - y.min()  # Shift values to be non-negative if needed\n",
        "            if len(y) == 0:  # Prevent division by zero\n",
        "               return 0\n",
        "            probs = np.bincount(y) / len(y)\n",
        "            return -np.sum([p * np.log2(p) for p in probs if p > 0])\n",
        "\n",
        "\n",
        "        n = len(parent)\n",
        "        return entropy(parent) - (len(left) / n * entropy(left) + len(right) / n * entropy(right))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if not isinstance(node, tuple):\n",
        "            return node\n",
        "        feature_idx, threshold, left, right = node\n",
        "        return self._traverse_tree(x, left) if x[feature_idx] < threshold else self._traverse_tree(x, right)\n",
        "\n",
        "# Manual implementation of Random Forest\n",
        "class RandomForest:\n",
        "    def __init__(self, n_estimators=10, max_depth=5, max_features=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.max_features = max_features\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            indices = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_sample, y_sample = X[indices], y[indices]\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.array([Counter(tree_predictions[:, i]).most_common(1)[0][0] for i in range(len(X))])\n",
        "\n",
        "# Manual implementation of Gradient Boosting\n",
        "class GradientBoosting:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.initial_prediction = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.initial_prediction = np.mean(y)\n",
        "        residuals = y - self.initial_prediction\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.full(X.shape[0], self.initial_prediction)\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "# Manual implementation of XGBoost\n",
        "class XGBoost:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        residuals = y.copy()\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.zeros(X.shape[0])\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "# Manual implementation of Ensemble Model with Majority Voting\n",
        "class EnsembleModel:\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for model in self.models:\n",
        "            model.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([model.predict(X) for model in self.models])\n",
        "        return np.array([Counter(predictions[:, i]).most_common(1)[0][0] for i in range(len(X))])\n",
        "\n",
        "# Initialize models\n",
        "knn = KNN(k=3)\n",
        "rf = RandomForest(n_estimators=10, max_depth=5, max_features=None)\n",
        "gb = GradientBoosting(n_estimators=50, learning_rate=0.1, max_depth=3)\n",
        "xgb = XGBoost(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "\n",
        "# Create ensemble model\n",
        "ensemble = EnsembleModel(models=[knn, dt, gb,xgb])\n",
        "\n",
        "# Train and evaluate models\n",
        "models = {'KNN': knn, 'RandomForest':rf, 'Gradient Boosting': gb, 'XGBoost': xgb, 'Ensemble': ensemble}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "QItEl7tw0PYb",
        "outputId": "03c4b285-e249-48b3-afee-da8511ab2bdc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before SMOTE: Counter({1: 1655, 2: 295, 3: 176})\n",
            "Class distribution after SMOTE: Counter({2: 1655, 1: 1655, 3: 1655})\n",
            "KNN Accuracy: 0.9507\n",
            "RandomForest Accuracy: 0.8963\n",
            "Gradient Boosting Accuracy: 0.6647\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UFuncTypeError",
          "evalue": "Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bd1a59593f4c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'KNN'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RandomForest'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gradient Boosting'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'XGBoost'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ensemble'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-bd1a59593f4c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresiduals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mresiduals\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"fetal_health.csv\")\n",
        "X = df.iloc[:, :-1].to_numpy()\n",
        "y = df.iloc[:, -1].to_numpy().astype(int)\n",
        "\n",
        "# Display class distribution before SMOTE\n",
        "print(\"Class distribution before SMOTE:\", Counter(y))\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "y_resampled = np.round(y_resampled).astype(int)  # Remove any floating points\n",
        "y_resampled = np.clip(y_resampled, 1, 3)  # Ensure values are only in [1, 2, 3]\n",
        "\n",
        "\n",
        "# Display class distribution after SMOTE\n",
        "print(\"Class distribution after SMOTE:\", Counter(y_resampled))\n",
        "print(\"Unique values after SMOTE:\", np.unique(y_resampled))\n",
        "\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Unique values in y_train:\", np.unique(y_train))\n",
        "print(\"Unique values in y_test:\", np.unique(y_test))\n",
        "\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Manual implementation of K-Nearest Neighbors (KNN)\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for x in X_test:\n",
        "            distances = np.linalg.norm(self.X_train - x, axis=1)\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "            most_common = Counter(k_nearest_labels).most_common(1)\n",
        "            predictions.append(most_common[0][0])\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Manual implementation of Decision Tree used in Random Forest\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1]\n",
        "        self.tree = self._grow_tree(X, y, depth=0)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth):\n",
        "        num_samples, num_features = X.shape\n",
        "        if depth >= self.max_depth or len(set(y)) == 1:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        best_feat, best_thresh = self._best_split(X, y)\n",
        "        if best_feat is None:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        left_idx = X[:, best_feat] < best_thresh\n",
        "        right_idx = ~left_idx\n",
        "        left_subtree = self._grow_tree(X[left_idx], y[left_idx], depth + 1)\n",
        "        right_subtree = self._grow_tree(X[right_idx], y[right_idx], depth + 1)\n",
        "\n",
        "        return (best_feat, best_thresh, left_subtree, right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "        for i in range(self.n_features):\n",
        "            thresholds = np.unique(X[:, i])\n",
        "            for threshold in thresholds:\n",
        "                left_idx = X[:, i] < threshold\n",
        "                right_idx = ~left_idx\n",
        "                if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
        "                    continue\n",
        "                gain = self._information_gain(y, y[left_idx], y[right_idx])\n",
        "                if gain > best_gain:\n",
        "                    best_gain, split_idx, split_threshold = gain, i, threshold\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "\n",
        "    def _entropy(self, y):\n",
        "       y = np.array(y, dtype=int)\n",
        "       if len(y) == 0:\n",
        "          return 0\n",
        "       if np.min(y) < 0:\n",
        "           raise ValueError(f\"Negative values found in y: {y}\")  # Debugging step\n",
        "       if np.max(y) > 3:\n",
        "           raise ValueError(f\"Unexpected high values found in y: {y}\")  # Another check\n",
        "       probs = np.bincount(y) / len(y)\n",
        "       return -np.sum(probs[probs > 0] * np.log2(probs[probs > 0]))\n",
        "\n",
        "\n",
        "    def _information_gain(self, parent, left, right):\n",
        "        n = len(parent)\n",
        "        return self._entropy(parent) - (len(left) / n * self._entropy(left) + len(right) / n * self._entropy(right))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if not isinstance(node, tuple):\n",
        "            return node\n",
        "        feature_idx, threshold, left, right = node\n",
        "        return self._traverse_tree(x, left) if x[feature_idx] < threshold else self._traverse_tree(x, right)\n",
        "\n",
        "# Manual implementation of Random Forest\n",
        "class RandomForest:\n",
        "    def __init__(self, n_estimators=10, max_depth=5, max_features=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.max_features = max_features\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            indices = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_sample, y_sample = X[indices], y[indices]\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.array([Counter(tree_predictions[:, i]).most_common(1)[0][0] for i in range(len(X))])\n",
        "\n",
        "# Manual implementation of Gradient Boosting\n",
        "class GradientBoosting:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.initial_prediction = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.initial_prediction = np.mean(y)\n",
        "        residuals = y - self.initial_prediction\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.full(X.shape[0], self.initial_prediction)\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "# Manual implementation of XGBoost\n",
        "class XGBoost:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        residuals = y.copy()\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.zeros(X.shape[0])\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "# Manual implementation of Ensemble Model with Majority Voting\n",
        "class EnsembleModel:\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for model in self.models:\n",
        "            model.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([model.predict(X) for model in self.models])\n",
        "        return np.array([Counter(predictions[:, i]).most_common(1)[0][0] for i in range(len(X))])\n",
        "\n",
        "# Initialize models\n",
        "knn = KNN(k=3)\n",
        "rf = RandomForest(n_estimators=10, max_depth=5, max_features=None)\n",
        "gb = GradientBoosting(n_estimators=50, learning_rate=0.1, max_depth=3)\n",
        "xgb = XGBoost(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "\n",
        "# Create ensemble model\n",
        "ensemble = EnsembleModel(models=[knn, dt, gb,xgb])\n",
        "\n",
        "# Train and evaluate models\n",
        "models = {'KNN': knn, 'RandomForest':rf, 'Gradient Boosting': gb, 'XGBoost': xgb, 'Ensemble': ensemble}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "_c0AeszH6qBZ",
        "outputId": "8e716339-2590-4469-d7fe-d262ace5b967"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before SMOTE: Counter({1: 1655, 2: 295, 3: 176})\n",
            "Class distribution after SMOTE: Counter({2: 1655, 1: 1655, 3: 1655})\n",
            "Unique values after SMOTE: [1 2 3]\n",
            "Unique values in y_train: [1 2 3]\n",
            "Unique values in y_test: [1 2 3]\n",
            "KNN Accuracy: 0.9507\n",
            "RandomForest Accuracy: 0.8943\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Negative values found in y: [ 0  0  0 ...  0  0 -1]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-eadc11a94624>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'KNN'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RandomForest'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gradient Boosting'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'XGBoost'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ensemble'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-eadc11a94624>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresiduals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mresiduals\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-eadc11a94624>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-eadc11a94624>\u001b[0m in \u001b[0;36m_grow_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mbest_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_best_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_feat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-eadc11a94624>\u001b[0m in \u001b[0;36m_best_split\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_information_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_gain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mbest_gain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-eadc11a94624>\u001b[0m in \u001b[0;36m_information_gain\u001b[0;34m(self, parent, left, right)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_information_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-eadc11a94624>\u001b[0m in \u001b[0;36m_entropy\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    105\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Negative values found in y: {y}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Debugging step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected high values found in y: {y}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Another check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative values found in y: [ 0  0  0 ...  0  0 -1]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"fetal_health.csv\")\n",
        "X = df.iloc[:, :-1].to_numpy()\n",
        "y = df.iloc[:, -1].to_numpy().astype(int)\n",
        "\n",
        "# Display class distribution before SMOTE\n",
        "print(\"Class distribution before SMOTE:\", Counter(y))\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "y_resampled = y_resampled.astype(int)\n",
        "\n",
        "# Debugging: Check for negative values in y_resampled\n",
        "print(\"Unique values in y_resampled:\", np.unique(y_resampled))\n",
        "assert np.all(y_resampled > 0), \"Error: Negative values found in y_resampled!\"\n",
        "\n",
        "# Display class distribution after SMOTE\n",
        "print(\"Class distribution after SMOTE:\", Counter(y_resampled))\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Debugging: Check for negative values in y_train\n",
        "print(\"Unique values in y_train:\", np.unique(y_train))\n",
        "assert np.all(y_train > 0), \"Error: Negative values found in y_train!\"\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Manual implementation of K-Nearest Neighbors (KNN)\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for x in X_test:\n",
        "            distances = np.linalg.norm(self.X_train - x, axis=1)\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "            most_common = Counter(k_nearest_labels).most_common(1)\n",
        "            predictions.append(most_common[0][0])\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Manual implementation of Decision Tree used in Random Forest\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1]\n",
        "        self.tree = self._grow_tree(X, y, depth=0)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth):\n",
        "        num_samples, num_features = X.shape\n",
        "        if depth >= self.max_depth or len(set(y)) == 1:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        best_feat, best_thresh = self._best_split(X, y)\n",
        "        if best_feat is None:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        left_idx = X[:, best_feat] < best_thresh\n",
        "        right_idx = ~left_idx\n",
        "        left_subtree = self._grow_tree(X[left_idx], y[left_idx], depth + 1)\n",
        "        right_subtree = self._grow_tree(X[right_idx], y[right_idx], depth + 1)\n",
        "\n",
        "        return (best_feat, best_thresh, left_subtree, right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "        for i in range(self.n_features):\n",
        "            thresholds = np.unique(X[:, i])\n",
        "            for threshold in thresholds:\n",
        "                left_idx = X[:, i] < threshold\n",
        "                right_idx = ~left_idx\n",
        "                if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
        "                    continue\n",
        "                gain = self._information_gain(y, y[left_idx], y[right_idx])\n",
        "                if gain > best_gain:\n",
        "                    best_gain, split_idx, split_threshold = gain, i, threshold\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _entropy(self, y):\n",
        "      y = np.array(y)\n",
        "      if len(y) == 0:\n",
        "         return 0\n",
        "      if np.min(y) < 1:  # Check for invalid class labels\n",
        "         raise ValueError(f\"Invalid class labels found in y: {y}\")\n",
        "      counts = np.bincount(y - 1)  # Adjust class labels for bincount\n",
        "      probs = counts / len(y)\n",
        "      return -np.sum(probs[probs > 0] * np.log2(probs[probs > 0]))\n",
        "\n",
        "\n",
        "    def _information_gain(self, parent, left, right):\n",
        "        n = len(parent)\n",
        "        return self._entropy(parent) - (len(left) / n * self._entropy(left) + len(right) / n * self._entropy(right))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if not isinstance(node, tuple):\n",
        "            return node\n",
        "        feature_idx, threshold, left, right = node\n",
        "        return self._traverse_tree(x, left) if x[feature_idx] < threshold else self._traverse_tree(x, right)\n",
        "\n",
        "# Manual implementation of Random Forest\n",
        "class RandomForest:\n",
        "    def __init__(self, n_estimators=10, max_depth=5, max_features=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.max_features = max_features\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            indices = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_sample, y_sample = X[indices], y[indices]\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.array([Counter(tree_predictions[:, i]).most_common(1)[0][0] for i in range(len(X))])\n",
        "# Manual implementation of Gradient Boosting\n",
        "class GradientBoosting:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.initial_prediction = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.initial_prediction = np.mean(y)\n",
        "        residuals = y - self.initial_prediction\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.full(X.shape[0], self.initial_prediction)\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "# Manual implementation of XGBoost\n",
        "class XGBoost:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        residuals = y.copy()\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.zeros(X.shape[0])\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "# Manual implementation of Ensemble Model with Majority Voting\n",
        "class EnsembleModel:\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for model in self.models:\n",
        "            model.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([model.predict(X) for model in self.models])\n",
        "        return np.array([Counter(predictions[:, i]).most_common(1)[0][0] for i in range(len(X))])\n",
        "\n",
        "# Initialize models\n",
        "knn = KNN(k=3)\n",
        "rf = RandomForest(n_estimators=10, max_depth=5, max_features=None)\n",
        "gb = GradientBoosting(n_estimators=50, learning_rate=0.1, max_depth=3)\n",
        "xgb = XGBoost(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "\n",
        "# Create ensemble model\n",
        "ensemble = EnsembleModel(models=[knn, dt, gb,xgb])\n",
        "\n",
        "# Train and evaluate models\n",
        "models = {'KNN': knn, 'RandomForest':rf, 'Gradient Boosting': gb, 'XGBoost': xgb, 'Ensemble': ensemble}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "ToXch4OkCKld",
        "outputId": "55382410-665a-49d6-9197-bdd02af14c5e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before SMOTE: Counter({1: 1655, 2: 295, 3: 176})\n",
            "Unique values in y_resampled: [1 2 3]\n",
            "Class distribution after SMOTE: Counter({2: 1655, 1: 1655, 3: 1655})\n",
            "Unique values in y_train: [1 2 3]\n",
            "KNN Accuracy: 0.9507\n",
            "RandomForest Accuracy: 0.8983\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid class labels found in y: [-0.00755287 -0.00755287  0.99244713 ... -0.00755287  0.99244713\n -1.00755287]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d70415038b54>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'KNN'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RandomForest'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gradient Boosting'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'XGBoost'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ensemble'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d70415038b54>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresiduals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mresiduals\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d70415038b54>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d70415038b54>\u001b[0m in \u001b[0;36m_grow_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mbest_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_best_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_feat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d70415038b54>\u001b[0m in \u001b[0;36m_best_split\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_information_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_gain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0mbest_gain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d70415038b54>\u001b[0m in \u001b[0;36m_information_gain\u001b[0;34m(self, parent, left, right)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_information_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d70415038b54>\u001b[0m in \u001b[0;36m_entropy\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    103\u001b[0m          \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Check for invalid class labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m          \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid class labels found in y: {y}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m       \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust class labels for bincount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid class labels found in y: [-0.00755287 -0.00755287  0.99244713 ... -0.00755287  0.99244713\n -1.00755287]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"fetal_health.csv\")\n",
        "X = df.iloc[:, :-1].to_numpy()\n",
        "y = df.iloc[:, -1].to_numpy().astype(int)\n",
        "\n",
        "# Display class distribution before SMOTE\n",
        "print(\"Class distribution before SMOTE:\", Counter(y))\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "y_resampled = y_resampled.astype(int)  # Ensure integer labels\n",
        "\n",
        "# Display class distribution after SMOTE\n",
        "print(\"Class distribution after SMOTE:\", Counter(y_resampled))\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale only X_train and X_test, NOT y_train\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Manual implementation of K-Nearest Neighbors (KNN)\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for x in X_test:\n",
        "            distances = np.linalg.norm(self.X_train - x, axis=1)\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "            most_common = Counter(k_nearest_labels).most_common(1)\n",
        "            predictions.append(most_common[0][0])\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Manual implementation of Decision Tree\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1]\n",
        "        self.tree = self._grow_tree(X, y, depth=0)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth):\n",
        "        num_samples, num_features = X.shape\n",
        "        if depth >= self.max_depth or len(set(y)) == 1:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        best_feat, best_thresh = self._best_split(X, y)\n",
        "        if best_feat is None:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        left_idx = X[:, best_feat] < best_thresh\n",
        "        right_idx = ~left_idx\n",
        "        left_subtree = self._grow_tree(X[left_idx], y[left_idx], depth + 1)\n",
        "        right_subtree = self._grow_tree(X[right_idx], y[right_idx], depth + 1)\n",
        "\n",
        "        return (best_feat, best_thresh, left_subtree, right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "        for i in range(self.n_features):\n",
        "            thresholds = np.unique(X[:, i])\n",
        "            for threshold in thresholds:\n",
        "                left_idx = X[:, i] < threshold\n",
        "                right_idx = ~left_idx\n",
        "                if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
        "                    continue\n",
        "                gain = self._information_gain(y, y[left_idx], y[right_idx])\n",
        "                if gain > best_gain:\n",
        "                    best_gain, split_idx, split_threshold = gain, i, threshold\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        y = np.array(y)\n",
        "        if len(y) == 0:\n",
        "            return 0\n",
        "        probs = np.bincount(y) / len(y)\n",
        "        probs = probs[probs > 0]  # Avoid log(0)\n",
        "        return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "    def _information_gain(self, parent, left, right):\n",
        "        n = len(parent)\n",
        "        return self._entropy(parent) - (len(left) / n * self._entropy(left) + len(right) / n * self._entropy(right))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if not isinstance(node, tuple):\n",
        "            return node\n",
        "        feature_idx, threshold, left, right = node\n",
        "        return self._traverse_tree(x, left) if x[feature_idx] < threshold else self._traverse_tree(x, right)\n",
        "\n",
        "# Manual implementation of Random Forest\n",
        "class RandomForest:\n",
        "    def __init__(self, n_estimators=10, max_depth=5):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_estimators):  # Fixed syntax error\n",
        "            indices = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_sample, y_sample = X[indices], y[indices]\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.array([Counter(tree_predictions[:, i]).most_common(1)[0][0] for i in range(len(X))])\n",
        "\n",
        "# Initialize models\n",
        "knn = KNN(k=3)\n",
        "dt = DecisionTree(max_depth=5)\n",
        "rf = RandomForest(n_estimators=10, max_depth=5)\n",
        "\n",
        "# Train and evaluate models\n",
        "models = {'KNN': knn, 'Decision Tree': dt, 'Random Forest': rf}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOpi74LOHcsD",
        "outputId": "453e050e-165b-4543-fb0a-66e725cb0e52"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before SMOTE: Counter({1: 1655, 2: 295, 3: 176})\n",
            "Class distribution after SMOTE: Counter({2: 1655, 1: 1655, 3: 1655})\n",
            "KNN Accuracy: 0.9507\n",
            "Decision Tree Accuracy: 0.8963\n",
            "Random Forest Accuracy: 0.8973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"fetal_health.csv\")\n",
        "X = df.iloc[:, :-1].to_numpy()\n",
        "y = df.iloc[:, -1].to_numpy().astype(int)\n",
        "\n",
        "# Display class distribution before SMOTE\n",
        "print(\"Class distribution before SMOTE:\", Counter(y))\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "y_resampled = y_resampled.astype(int)  # Ensure integer labels\n",
        "\n",
        "# Display class distribution after SMOTE\n",
        "print(\"Class distribution after SMOTE:\", Counter(y_resampled))\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale only X_train and X_test, NOT y_train\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Manual implementation of K-Nearest Neighbors (KNN)\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for x in X_test:\n",
        "            distances = np.linalg.norm(self.X_train - x, axis=1)\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "            most_common = Counter(k_nearest_labels).most_common(1)\n",
        "            predictions.append(most_common[0][0])\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Manual implementation of Decision Tree\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1]\n",
        "        self.tree = self._grow_tree(X, y, depth=0)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth):\n",
        "        num_samples, num_features = X.shape\n",
        "        if depth >= self.max_depth or len(set(y)) == 1:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        best_feat, best_thresh = self._best_split(X, y)\n",
        "        if best_feat is None:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        left_idx = X[:, best_feat] < best_thresh\n",
        "        right_idx = ~left_idx\n",
        "        left_subtree = self._grow_tree(X[left_idx], y[left_idx], depth + 1)\n",
        "        right_subtree = self._grow_tree(X[right_idx], y[right_idx], depth + 1)\n",
        "\n",
        "        return (best_feat, best_thresh, left_subtree, right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "        for i in range(self.n_features):\n",
        "            thresholds = np.unique(X[:, i])\n",
        "            for threshold in thresholds:\n",
        "                left_idx = X[:, i] < threshold\n",
        "                right_idx = ~left_idx\n",
        "                if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
        "                    continue\n",
        "                gain = self._information_gain(y, y[left_idx], y[right_idx])\n",
        "                if gain > best_gain:\n",
        "                    best_gain, split_idx, split_threshold = gain, i, threshold\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _entropy(self, y):\n",
        "       y = np.array(y, dtype=int)  # Convert to integer\n",
        "       if len(y) == 0:\n",
        "          return 0\n",
        "       unique_classes, counts = np.unique(y, return_counts=True)\n",
        "       probs = counts / len(y)\n",
        "       return -np.sum(probs * np.log2(probs))  # Compute entropy safely\n",
        "\n",
        "\n",
        "    def _information_gain(self, parent, left, right):\n",
        "        n = len(parent)\n",
        "        return self._entropy(parent) - (len(left) / n * self._entropy(left) + len(right) / n * self._entropy(right))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if not isinstance(node, tuple):\n",
        "            return node\n",
        "        feature_idx, threshold, left, right = node\n",
        "        return self._traverse_tree(x, left) if x[feature_idx] < threshold else self._traverse_tree(x, right)\n",
        "\n",
        "# Manual implementation of Random Forest\n",
        "class RandomForest:\n",
        "    def __init__(self, n_estimators=10, max_depth=5):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_estimators):  # Fixed syntax error\n",
        "            indices = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_sample, y_sample = X[indices], y[indices]\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.array([Counter(tree_predictions[:, i]).most_common(1)[0][0] for i in range(len(X))])\n",
        "\n",
        "# Manual implementation of Gradient Boosting\n",
        "class GradientBoosting:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.initial_prediction = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.initial_prediction = np.mean(y)\n",
        "        residuals = y - self.initial_prediction\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.full(X.shape[0], self.initial_prediction)\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "# Manual implementation of XGBoost\n",
        "class XGBoost:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        residuals = y.copy()\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions = tree.predict(X)\n",
        "            residuals -= self.learning_rate * predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.zeros(X.shape[0])\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "# Manual implementation of Ensemble Model with Majority Voting\n",
        "class EnsembleModel:\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for model in self.models:\n",
        "            model.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([model.predict(X) for model in self.models])\n",
        "        return np.array([Counter(predictions[:, i]).most_common(1)[0][0] for i in range(len(X))])\n",
        "\n",
        "# Initialize models\n",
        "knn = KNN(k=3)\n",
        "rf = RandomForest(n_estimators=10, max_depth=5)\n",
        "gb = GradientBoosting(n_estimators=50, learning_rate=0.1, max_depth=3)\n",
        "xgb = XGBoost(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "\n",
        "# Create ensemble model\n",
        "ensemble = EnsembleModel(models=[knn, rf, gb,xgb])\n",
        "\n",
        "# Train and evaluate models\n",
        "models = {'KNN': knn, 'RandomForest':rf, 'Gradient Boosting': gb, 'XGBoost': xgb, 'Ensemble': ensemble}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "syInPlkdH99e",
        "outputId": "84258815-36d8-490e-d676-4ca93964054a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before SMOTE: Counter({1: 1655, 2: 295, 3: 176})\n",
            "Class distribution after SMOTE: Counter({2: 1655, 1: 1655, 3: 1655})\n",
            "KNN Accuracy: 0.9507\n",
            "RandomForest Accuracy: 0.8963\n",
            "Gradient Boosting Accuracy: 0.6647\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UFuncTypeError",
          "evalue": "Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-29f808af0f1e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'KNN'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RandomForest'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gradient Boosting'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'XGBoost'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ensemble'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-29f808af0f1e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresiduals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mresiduals\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-xxCt8dNbtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}