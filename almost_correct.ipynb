{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-d11r62ldqP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"fetal_health.csv\")\n",
        "\n",
        "# Define features and target\n",
        "X = df.iloc[:, :-1].to_numpy()\n",
        "y = df.iloc[:, -1].to_numpy().astype(int)\n",
        "\n",
        "# Display original class distribution\n",
        "print(\"Original Class Distribution:\", Counter(y))\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Convert y_resampled to integer type (SMOTE might introduce float labels)\n",
        "y_resampled = y_resampled.astype(int)\n",
        "\n",
        "# Display new class distribution\n",
        "print(\"After SMOTE Class Distribution:\", Counter(y_resampled))\n",
        "\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# ✅ 1. Manual Decision Tree\n",
        "# --------------------------------------\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "class ManualDecisionTree:\n",
        "    def __init__(self, max_depth=10, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        num_samples, num_features = X.shape\n",
        "        unique_classes = np.unique(y)\n",
        "\n",
        "        if depth >= self.max_depth or len(unique_classes) == 1 or num_samples < self.min_samples_split:\n",
        "            return Node(value=Counter(y).most_common(1)[0][0])\n",
        "\n",
        "        best_feature, best_threshold = self._best_split(X, y)\n",
        "        if best_feature is None:\n",
        "            return Node(value=Counter(y).most_common(1)[0][0])\n",
        "\n",
        "        left_idx = X[:, best_feature] <= best_threshold\n",
        "        right_idx = X[:, best_feature] > best_threshold\n",
        "\n",
        "        left_subtree = self._build_tree(X[left_idx], y[left_idx], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_idx], y[right_idx], depth + 1)\n",
        "\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain = -1\n",
        "        best_feature, best_threshold = None, None\n",
        "        for feature in range(X.shape[1]):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for threshold in thresholds:\n",
        "                left_idx = X[:, feature] <= threshold\n",
        "                right_idx = X[:, feature] > threshold\n",
        "                if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
        "                    continue\n",
        "                gain = self._information_gain(y, y[left_idx], y[right_idx])\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _entropy(self, y):  # ✅ Add 'self' as the first argument\n",
        "       values, counts = np.unique(y, return_counts=True)  # Works with floats too\n",
        "       probabilities = counts / len(y)\n",
        "       return -np.sum(probabilities * np.log2(probabilities + 1e-9))\n",
        "\n",
        "\n",
        "    def _information_gain(self, y, y_left, y_right):\n",
        "        return self._entropy(y) - (len(y_left) / len(y) * self._entropy(y_left) + len(y_right) / len(y) * self._entropy(y_right))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict(x, self.tree) for x in X])\n",
        "\n",
        "    def _predict(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        return self._predict(x, node.left if x[node.feature] <= node.threshold else node.right)\n",
        "\n",
        "# --------------------------------------\n",
        "# ✅ 2. Manual K-Nearest Neighbors (KNN)\n",
        "# --------------------------------------\n",
        "class ManualKNN:\n",
        "    def __init__(self, k=5):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            distances = np.linalg.norm(self.X_train - x, axis=1)\n",
        "            k_neighbors = np.argsort(distances)[:self.k]\n",
        "            labels = [self.y_train[i] for i in k_neighbors]\n",
        "            predictions.append(Counter(labels).most_common(1)[0][0])\n",
        "        return np.array(predictions)\n",
        "\n",
        "# --------------------------------------\n",
        "# ✅ 3. Manual Random Forest\n",
        "# --------------------------------------\n",
        "class ManualRandomForest:\n",
        "    def __init__(self, n_trees=10, max_depth=5, min_samples_split=2):\n",
        "        self.n_trees = n_trees\n",
        "        self.trees = [ManualDecisionTree(max_depth, min_samples_split) for _ in range(n_trees)]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for tree in self.trees:\n",
        "            idxs = np.random.choice(len(X), len(X), replace=True)\n",
        "            tree.fit(X[idxs], y[idxs])\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
        "\n",
        "# --------------------------------------\n",
        "# ✅ 4. Manual Gradient Boosting\n",
        "# --------------------------------------\n",
        "class ManualGradientBoosting:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        residuals = y.astype(np.float64)\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = ManualDecisionTree(max_depth=7)\n",
        "            tree.fit(X, residuals)\n",
        "            self.models.append(tree)\n",
        "            residuals -= self.learning_rate * tree.predict(X)\n",
        "    def predict(self, X):\n",
        "        preds = np.sum([self.learning_rate * tree.predict(X) for tree in self.models], axis=0)\n",
        "        return np.round(preds).astype(int)\n",
        "\n",
        "class ManualXGBoost:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        predictions = np.zeros(len(y))\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - predictions  # Compute residuals (negative gradient)\n",
        "            tree = ManualDecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            self.models.append(tree)\n",
        "            predictions += self.learning_rate * tree.predict(X)  # Update predictions\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = np.sum([self.learning_rate * tree.predict(X) for tree in self.models], axis=0)\n",
        "        return np.round(preds).astype(int)\n",
        "\n",
        "\n",
        "# ✅ Train & Evaluate Models\n",
        "models = {\n",
        "    \"Decision Tree\": ManualDecisionTree(),\n",
        "    \"KNN\": ManualKNN(k=5),\n",
        "    \"Random Forest\": ManualRandomForest(),\n",
        "    \"Gradient Boosting\": ManualGradientBoosting(),\n",
        "    \"XGBoost\": ManualXGBoost()\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    print(f\"\\n{name} - Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.show()\n"
      ]
    }
  ]
}