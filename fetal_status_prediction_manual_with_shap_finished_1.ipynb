{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlDWALpJvR4U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"fetal_health.csv\")\n",
        "\n",
        "# Define features and target\n",
        "X = df.iloc[:, :-1].to_numpy()  # Features\n",
        "y = df.iloc[:, -1].to_numpy().astype(int)  # Ensure integer class labels\n",
        "\n",
        "# Display original class distribution\n",
        "print(\"Original Class Distribution:\", Counter(y))\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Convert y_resampled to integer type (SMOTE might introduce float labels)\n",
        "y_resampled = y_resampled.astype(int)\n",
        "\n",
        "# Display new class distribution\n",
        "print(\"After SMOTE Class Distribution:\", Counter(y_resampled))\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --------------------------------------\n",
        "# ✅ 1. Manual K-Nearest Neighbors (KNN)\n",
        "# --------------------------------------\n",
        "class ManualKNN:\n",
        "    def __init__(self, k=5):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            distances = np.linalg.norm(self.X_train - x, axis=1)  # Efficient vectorized distance calculation\n",
        "            k_neighbors = np.argsort(distances)[:self.k]\n",
        "            labels = [self.y_train[i] for i in k_neighbors]\n",
        "            predictions.append(Counter(labels).most_common(1)[0][0])\n",
        "        return np.array(predictions)\n",
        "\n",
        "# --------------------------------------\n",
        "# ✅ 2. Manual XGBoost\n",
        "# --------------------------------------\n",
        "class XGBoostManual:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.initial_prediction = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.initial_prediction = np.mean(y)\n",
        "        y_pred = np.full(y.shape, self.initial_prediction)\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - y_pred\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.full((X.shape[0],), self.initial_prediction)\n",
        "        for tree in self.trees:\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(y_pred).astype(int)\n",
        "\n",
        "# --------------------------------------\n",
        "# ✅ 3. Manual Random Forest\n",
        "# --------------------------------------\n",
        "class ManualRandomForest:\n",
        "    def __init__(self, n_trees=50, max_depth=10):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.n_trees):\n",
        "            indices = np.random.choice(np.arange(len(X)), size=len(X), replace=True)\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X[indices], y[indices])\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.round(np.mean(predictions, axis=0)).astype(int)\n",
        "\n",
        "# --------------------------------------\n",
        "# ✅ 4. Manual Gradient Boosting\n",
        "# --------------------------------------\n",
        "class ManualGradientBoosting:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.initial_prediction = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.initial_prediction = np.mean(y)\n",
        "        y_pred = np.full(y.shape, self.initial_prediction)\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - y_pred\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.full((X.shape[0],), self.initial_prediction)\n",
        "        for tree in self.trees:\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "        return np.round(y_pred).astype(int)\n",
        "\n",
        "# --------------------------------------\n",
        "# ✅ Train and Evaluate Models\n",
        "# --------------------------------------\n",
        "models = {\n",
        "    \"KNN\": ManualKNN(k=5),\n",
        "    \"XGBoost\": XGBoostManual(n_estimators=100, learning_rate=0.1, max_depth=3),\n",
        "    \"Random Forest\": ManualRandomForest(n_trees=50, max_depth=10),\n",
        "    \"Gradient Boosting\": ManualGradientBoosting(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "}\n",
        "\n",
        "predictions = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    predictions[name] = y_pred\n",
        "    print(f\"\\n{name} - Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[1, 2, 3], yticklabels=[1, 2, 3])\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.show()\n",
        "\n",
        "# ✅ Manual Ensembling (Majority Voting)\n",
        "y_pred_ensemble = np.array([Counter([predictions[model][i] for model in models]).most_common(1)[0][0] for i in range(len(y_test))])\n",
        "\n",
        "# Evaluate ensemble model\n",
        "print(f\"\\nEnsemble Model - Accuracy: {accuracy_score(y_test, y_pred_ensemble) * 100:.2f}%\")\n",
        "print(classification_report(y_test, y_pred_ensemble))\n",
        "\n",
        "# Plot Confusion Matrix for Ensemble Model\n",
        "cm_ensemble = confusion_matrix(y_test, y_pred_ensemble)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm_ensemble, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[1, 2, 3], yticklabels=[1, 2, 3])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - Ensemble Model\")\n",
        "plt.show()\n",
        "\n",
        "new_data_class_2 = pd.DataFrame({\n",
        "    'baseline value': [120],  # Higher baseline value, could be indicative of a different class\n",
        "    'accelerations': [0],  # Acceleration might be higher for class 2\n",
        "    'fetal_movement': [0],  # Slightly more fetal movement\n",
        "    'uterine_contractions': [0],  # Uterine contractions might be more frequent for class 2\n",
        "    'light_decelerations': [0],  # Slightly more decelerations\n",
        "    'severe_decelerations': [0],  # Mild severe deceleration\n",
        "    'prolongued_decelerations': [0],  # Prolonged decelerations slightly higher\n",
        "    'abnormal_short_term_variability': [73],  # Short term variability higher\n",
        "    'mean_value_of_short_term_variability': [0.5],  # Increased mean value\n",
        "    'percentage_of_time_with_abnormal_long_term_variability': [43],  # More time with abnormal long-term variability\n",
        "    'mean_value_of_long_term_variability': [2.4],  # Increased mean value for long-term variability\n",
        "    'histogram_width': [64],  # Wider histogram could indicate different characteristics\n",
        "    'histogram_min': [62],\n",
        "    'histogram_max': [126],  # Higher maximum could indicate a different class\n",
        "    'histogram_number_of_peaks': [2],  # More peaks in the histogram\n",
        "    'histogram_number_of_zeroes': [0],  # More zeros might correlate with different patterns\n",
        "    'histogram_mode': [120],  # Mode might be higher\n",
        "    'histogram_mean': [137],  # Higher mean value\n",
        "    'histogram_median': [121],  # Higher median\n",
        "    'histogram_variance': [73],  # More variance\n",
        "    'histogram_tendency': [1]  # Slightly more tendency\n",
        "})\n",
        "# Class labels mapping\n",
        "class_labels = {1: 'Normal', 2: 'Suspect', 3: 'Pathological'}\n",
        "\n",
        "# Scale the new data using the fitted scaler\n",
        "# Convert DataFrame to NumPy array before scaling\n",
        "new_data_scaled = scaler.transform(new_data_class_2.to_numpy())\n",
        "\n",
        "# Predict using individual models\n",
        "model_predictions = []\n",
        "for name, model in models.items():\n",
        "    pred = model.predict(new_data_scaled)\n",
        "    model_predictions.append(pred[0])\n",
        "\n",
        "# Majority voting (Ensemble Prediction)\n",
        "final_prediction = Counter(model_predictions).most_common(1)[0][0]\n",
        "\n",
        "# Get class name from class_labels dictionary\n",
        "predicted_class_name = class_labels[final_prediction]\n",
        "\n",
        "# Print final result\n",
        "print(f\"Predicted Class Number: {final_prediction}\")\n",
        "print(f\"Predicted Class Name: {predicted_class_name}\")\n",
        "\n",
        "import shap\n",
        "\n",
        "# Initialize SHAP explainer and compute SHAP values for each model\n",
        "shap_values_dict = {}\n",
        "explainer_dict = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    explainer = shap.Explainer(model.predict, X_train_scaled)\n",
        "    shap_values = explainer(X_test_scaled)\n",
        "    shap_values_dict[name] = shap_values\n",
        "    explainer_dict[name] = explainer\n",
        "\n",
        "    # Plot SHAP summary plot\n",
        "    shap.summary_plot(shap_values, X_test_scaled, feature_names=df.columns[:-1])\n",
        "\n",
        "# SHAP for Ensemble Model\n",
        "ensemble_explainer = shap.Explainer(lambda X: np.array([Counter([predictions[model][i] for model in models]).most_common(1)[0][0] for i in range(len(X))]), X_train_scaled)\n",
        "shap_values_ensemble = ensemble_explainer(X_test_scaled)\n",
        "shap.summary_plot(shap_values_ensemble, X_test_scaled, feature_names=df.columns[:-1])\n"
      ]
    }
  ]
}